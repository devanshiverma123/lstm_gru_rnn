{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4486f89",
   "metadata": {
    "id": "b18d6a30"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from gensim import utils\n",
    "import gensim.downloader as api\n",
    "import gensim.models\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8d60c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f5e5804",
    "outputId": "9cbff88f-c1a7-4cbf-bd0f-8c6137ebb36f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3072: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"amazon_reviews_us_Beauty_v1_00.tsv\", sep = '\\t', error_bad_lines=False,usecols = ['star_rating', 'review_body'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea78ac5d",
   "metadata": {
    "id": "9fe98f74"
   },
   "outputs": [],
   "source": [
    "class_1 = data.loc[data['star_rating'].isin([\"1\", \"2\"])]\n",
    "class_2 = data.loc[data['star_rating'].isin([\"3\"])]\n",
    "class_3 = data.loc[data['star_rating'].isin([\"4\", \"5\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e071c0",
   "metadata": {
    "id": "f2460c87"
   },
   "outputs": [],
   "source": [
    "class_1 = class_1.sample(n = 20000, random_state = 2)\n",
    "class_2 = class_2.sample(n = 20000, random_state = 2)\n",
    "class_3 = class_3.sample(n = 20000, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee470603",
   "metadata": {
    "id": "7618ea9d"
   },
   "outputs": [],
   "source": [
    "class_1['output_class'] = '1'\n",
    "class_2['output_class'] = '2'\n",
    "class_3['output_class'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d7851c6",
   "metadata": {
    "id": "d3881ba6"
   },
   "outputs": [],
   "source": [
    "complete_data = class_1.append(class_2)\n",
    "complete_data = complete_data.append(class_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5065f5b7",
   "metadata": {
    "id": "35e776e1"
   },
   "outputs": [],
   "source": [
    "complete_data = complete_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d9dc7e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ce65c9f0",
    "outputId": "577d4a89-371b-4cff-eb4e-443455f4fa17",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>output_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hmmm, where do I start? First of all I have be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Don't buy these dull pathetic shears.  Tried t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Did the company change the formula? This is my...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>leaky bottle in shipment. so many people sell ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>I saw this and thought it'd be worth a try; ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent product. Works great on my skin.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>5</td>\n",
       "      <td>Great price - great blades.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>5</td>\n",
       "      <td>This hair was AWESOME! It wasn't too thick/ po...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>5</td>\n",
       "      <td>My wife loves the product and actually put som...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>5</td>\n",
       "      <td>Bought as a replacement for an older (similar)...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      star_rating                                        review_body  \\\n",
       "0               1  Hmmm, where do I start? First of all I have be...   \n",
       "1               1  Don't buy these dull pathetic shears.  Tried t...   \n",
       "2               1  Did the company change the formula? This is my...   \n",
       "3               1  leaky bottle in shipment. so many people sell ...   \n",
       "4               2  I saw this and thought it'd be worth a try; ho...   \n",
       "...           ...                                                ...   \n",
       "59995           5         Excellent product. Works great on my skin.   \n",
       "59996           5                        Great price - great blades.   \n",
       "59997           5  This hair was AWESOME! It wasn't too thick/ po...   \n",
       "59998           5  My wife loves the product and actually put som...   \n",
       "59999           5  Bought as a replacement for an older (similar)...   \n",
       "\n",
       "      output_class  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "...            ...  \n",
       "59995            3  \n",
       "59996            3  \n",
       "59997            3  \n",
       "59998            3  \n",
       "59999            3  \n",
       "\n",
       "[60000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696f63e",
   "metadata": {
    "id": "5e9975d3"
   },
   "source": [
    "### 2. Word Embedding (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f4ef3",
   "metadata": {
    "id": "79f89f15"
   },
   "source": [
    "### (a) Load the pretrained Google News word2vec model and learn how to extract word embeddings for your dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81cc2d25",
   "metadata": {
    "id": "e5d12c2b"
   },
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f2e5bb79",
   "metadata": {
    "id": "b6c9e1db",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73005176\n"
     ]
    }
   ],
   "source": [
    "vector1 = wv['king'] - wv['man'] + wv['woman']\n",
    "vector2 = wv['queen']\n",
    "\n",
    "cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "049d40b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5567486\n"
     ]
    }
   ],
   "source": [
    "vector1 = wv['excellent']\n",
    "vector2 = wv['outstanding']\n",
    "\n",
    "cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9618fca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fc1f4c9",
    "outputId": "2ddec2b3-a696-4b85-9f47-b5012ba2fa55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'excellent'\t'outstanding'\t0.56\n",
      "'king'\t'queen'\t0.65\n",
      "'woman'\t'queen'\t0.32\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('excellent', 'outstanding'),  \n",
    "    ('king', 'queen'),\n",
    "    ('woman', 'queen'),\n",
    "   \n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1764b9",
   "metadata": {
    "id": "186e4d73"
   },
   "source": [
    "### (b) Train a Word2Vec model using your own dataset. You will use these extracted features in the subsequent questions of this assignment. Set the embedding size to be 300 and the window size to be 13. You can also consider a minimum word count of 9. Check the semantic similarities for the same two examples in part (a). What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better? For the rest of this assignment, use the pretrained “word2vec-google news-300” Word2Vec features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e336c3",
   "metadata": {
    "id": "9db5a853"
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = data['review_body'].dropna()\n",
    "        for line in corpus_path:\n",
    "#             print(line)\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92db2d53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "daa7cb04",
    "outputId": "163152bc-43b4-4f7f-84a3-090e7fe75e69"
   },
   "outputs": [],
   "source": [
    "sentences = MyCorpus()\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=300,\n",
    "    min_count=9,\n",
    "    window = 13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2fe5d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3874093\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# cosine_similarity(model_w2v.wv[\"king\"].reshape(-1, 1), model_w2v.wv[\"queen\"].reshape(-1, 1))\n",
    "vector1 = model_w2v.wv[\"king\"] - model_w2v.wv[\"man\"] + model_w2v.wv[\"woman\"]\n",
    "vector2 =  model_w2v.wv[\"queen\"]\n",
    "cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aea35fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83489233\n"
     ]
    }
   ],
   "source": [
    "vector1 = model_w2v.wv[\"excellent\"]\n",
    "vector2 =  model_w2v.wv[\"outstanding\"]\n",
    "cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c9c82",
   "metadata": {},
   "source": [
    "### Q) What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
    "\n",
    "### A) Comparing the vectors generated from the pretrained model and my own model, I observed that the pretrained model encodes the semantic similarities between the words better than my model in general. The reason can the be the dataset used for the pretrained model is huge.\n",
    "\n",
    "#### King − Man + Woman = Queen\n",
    "Similarity for Google Word2Vec - 0.73005176\n",
    "\n",
    "Similarity for My Model - 0.3874093\n",
    "\n",
    "#### Outstanding ~ Excellent\n",
    "\n",
    "Similarity for Google Word2Vec - 0.5567486\n",
    "\n",
    "Similarity for My Model - 0.83489233\n",
    "\n",
    "#### For the example Outstanding ~ Excellent our model performs better. The reason might be that our dataset consists of reviews, in which these words are very common and indicate the same sentiment. This semantic similarity was captured well by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3655268",
   "metadata": {
    "id": "JNErIlnJZjA2"
   },
   "source": [
    "### 3. Simple models (20 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b236958",
   "metadata": {
    "id": "5dd86c20"
   },
   "source": [
    "### Using the Google pre-trained Word2Vec features, train a single perceptron and an SVM model for the classification problem. For this purpose, use the average Word2Vec vectors for each review as the input feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81579e7a",
   "metadata": {
    "id": "3a51932c"
   },
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "\n",
    "    data['review_body'] = data['review_body'].dropna().apply(lambda x: x.lower())\n",
    "    data['review_body'] = data['review_body'].dropna().apply(lambda x: x.split())\n",
    "    data['review_body'] = data['review_body'].dropna().apply(lambda x: [word for word in x if word in wv])\n",
    "    data['review_body'] = data['review_body'].dropna().apply(lambda x: np.mean([wv[word] for word in x], axis = 0))\n",
    "    data.dropna(subset=['review_body'], inplace=True)\n",
    "\n",
    "    X = np.vstack(data['review_body'].values)\n",
    "    y = data['output_class'].values\n",
    "    \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8113ecc",
   "metadata": {
    "id": "a2c64c4b"
   },
   "outputs": [],
   "source": [
    "single_perceptron_data = complete_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1915b10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c377902",
    "outputId": "ffce7133-44ef-4e02-9259-6137e2aff997",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X, y = extract_features(single_perceptron_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "484e9b0c",
   "metadata": {
    "id": "7d71e006"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d636dc3",
   "metadata": {
    "id": "b77aa270"
   },
   "outputs": [],
   "source": [
    "# Define the perceptron model\n",
    "\n",
    "perceptron = Perceptron()\n",
    "\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test data\n",
    "y_pred = perceptron.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dfbf14e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f26d49e9",
    "outputId": "b7530f5e-cb3d-4dfb-b04b-7bb71a87f603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron results from word2vec embeddings\n",
      "Accuracy: 0.5959180245254494\n",
      "Precision 0.6364002047528308\n",
      "Recall 0.5959180245254494\n",
      "F1 0.5890829957518432\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron results from word2vec embeddings\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision\", precision)\n",
    "print(\"Recall\", recall)\n",
    "print(\"F1\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b0aba2b",
   "metadata": {
    "id": "ac531fa0"
   },
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the model on the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2976458",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42348a3a",
    "outputId": "6f4b3ccb-d80c-49d9-cb5f-a6249d68aab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM results from word2vec embeddings\n",
      "Accuracy: 0.6500923903913993\n",
      "Precision 0.6574941067400007\n",
      "Recall 0.6500923903913993\n",
      "F1 0.6526496371540883\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM results from word2vec embeddings\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision\", precision)\n",
    "print(\"Recall\", recall)\n",
    "print(\"F1\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8920de",
   "metadata": {},
   "source": [
    "### Q) What do you conclude from comparing performances for the models trained using the two different feature types \n",
    "\n",
    "### In HW1,  the TF-IDF embedding used generated the following results on test data:\n",
    "1.  Accuracy for Perceptron - 0.6070\n",
    "2. Accuracy for SVM - 0.65\n",
    "\n",
    "### On using Word2Vec embeddings in this assignment the results generated are as follows:\n",
    "1. Accuracy for Perceptron - 0.5959\n",
    "2. Accuracy for SVM - 0.65\n",
    "\n",
    "### On comparing both these results, we can say that the results are pretty similar for both the embeddings. Both thes embeddings give very similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded45308",
   "metadata": {
    "id": "Olz7jciXaVgq"
   },
   "source": [
    "### 4. Feedforward Neural Networks (25 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ceb26b04",
   "metadata": {
    "id": "4d5c3c1d"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c5f61",
   "metadata": {
    "id": "Vf7fG8jFaYk6"
   },
   "source": [
    "### (a) (10 points)\n",
    "To generate the input features, use the average Word2Vec vectors similar to\n",
    "the “Simple models” section and train the neural network. Report accuracy\n",
    "values on the testing split for your MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "675e6fa8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "731ec1a8",
    "outputId": "bc1af0c8-68de-416e-d870-46ed7c7082fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8669 - accuracy: 0.6102\n",
      "Epoch 2/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8330 - accuracy: 0.6376\n",
      "Epoch 3/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8267 - accuracy: 0.6411\n",
      "Epoch 4/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8223 - accuracy: 0.6474\n",
      "Epoch 5/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8216 - accuracy: 0.6462\n",
      "Epoch 6/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8198 - accuracy: 0.6467\n",
      "Epoch 7/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8196 - accuracy: 0.6469\n",
      "Epoch 8/20\n",
      "1489/1489 [==============================] - 1s 977us/step - loss: 0.8188 - accuracy: 0.6488\n",
      "Epoch 9/20\n",
      "1489/1489 [==============================] - 1s 986us/step - loss: 0.8184 - accuracy: 0.6488\n",
      "Epoch 10/20\n",
      "1489/1489 [==============================] - 1s 994us/step - loss: 0.8170 - accuracy: 0.6495\n",
      "Epoch 11/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8171 - accuracy: 0.6510\n",
      "Epoch 12/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8160 - accuracy: 0.6525\n",
      "Epoch 13/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8161 - accuracy: 0.6511\n",
      "Epoch 14/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8160 - accuracy: 0.6509\n",
      "Epoch 15/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8149 - accuracy: 0.6522\n",
      "Epoch 16/20\n",
      "1489/1489 [==============================] - 3s 2ms/step - loss: 0.8150 - accuracy: 0.6504\n",
      "Epoch 17/20\n",
      "1489/1489 [==============================] - 3s 2ms/step - loss: 0.8144 - accuracy: 0.6505\n",
      "Epoch 18/20\n",
      "1489/1489 [==============================] - 2s 1ms/step - loss: 0.8145 - accuracy: 0.6512\n",
      "Epoch 19/20\n",
      "1489/1489 [==============================] - 3s 2ms/step - loss: 0.8144 - accuracy: 0.6509\n",
      "Epoch 20/20\n",
      "1489/1489 [==============================] - 3s 2ms/step - loss: 0.8134 - accuracy: 0.6533\n",
      "373/373 [==============================] - 0s 815us/step - loss: 0.8152 - accuracy: 0.6480\n",
      "Accuracy and Loss for test data for FNN using word2vec embeddings.\n",
      "Test loss: 0.8151993751525879\n",
      "Test accuracy: 0.6479926109313965\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100,  input_shape=(300,)),\n",
    "    tf.keras.layers.Dense(10 ),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with cross-entropy loss and Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model with a batch size of 32 and for 10 epochs\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy and Loss for test data for FNN using word2vec embeddings.\")\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe1f651",
   "metadata": {
    "id": "28F639ZuaeyG"
   },
   "source": [
    "### (b) (15 points)\n",
    "To generate the input features, concatenate the first 10 Word2Vec vectors\n",
    "for each review as the input feature (x = [WT\n",
    "1\n",
    ", ..., WT\n",
    "10]) and train the neural\n",
    "network. Report the accuracy value on the testing split for your MLP model.\n",
    "What do you conclude by comparing accuracy values you obtain with\n",
    "those obtained in the “’Simple Models” section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8835c892",
   "metadata": {
    "id": "OgLoYZSIgTO-"
   },
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(df, model, num_words=10):\n",
    "\n",
    "    embeddings = []\n",
    "    for sentence in df['review_body']:\n",
    "        \n",
    "        if type(sentence) != float:\n",
    "            words = sentence.split()[:num_words]  # Take the first num_words words in the sentence\n",
    "            sentence_embeddings = []\n",
    "            for word in words:\n",
    "                if word in model:\n",
    "                    sentence_embeddings.append(model[word])\n",
    "                else:\n",
    "                    sentence_embeddings.append(np.zeros(300))\n",
    "            # Pad the sentence_embeddings list with zero vectors if necessary\n",
    "            while len(sentence_embeddings) < num_words:\n",
    "                sentence_embeddings.append(np.zeros(300))\n",
    "            \n",
    "            embeddings.append(np.concatenate(sentence_embeddings))\n",
    "        else:\n",
    "            sentence_embeddings = []\n",
    "            while len(sentence_embeddings) < num_words:\n",
    "                sentence_embeddings.append(np.zeros(300))\n",
    "            \n",
    "            embeddings.append(np.concatenate(sentence_embeddings))\n",
    "            \n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b2bfe5c",
   "metadata": {
    "id": "E-S1rYVqVXXy"
   },
   "outputs": [],
   "source": [
    "muti_percp_data = complete_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee60ef24",
   "metadata": {
    "id": "lUd1yDlVgY6X"
   },
   "outputs": [],
   "source": [
    "data_array = get_sentence_embeddings(muti_percp_data, wv, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bdfea87",
   "metadata": {
    "id": "iB6jn_LEdCwz"
   },
   "outputs": [],
   "source": [
    "X_10 = data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8ac99f0",
   "metadata": {
    "id": "ysaxyFCRiJiz"
   },
   "outputs": [],
   "source": [
    "y_10 = np.array(complete_data['output_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2537dfa3",
   "metadata": {
    "id": "YypxQzV5iy2U"
   },
   "outputs": [],
   "source": [
    "X_train_10, X_test_10, y_train_10, y_test_10 = train_test_split(X_10, y_10, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "557267e6",
   "metadata": {
    "id": "AfUc1XKfjG9n"
   },
   "outputs": [],
   "source": [
    "# Take 10 words.\n",
    "y_train_10 = to_categorical(y_train_10)\n",
    "y_test_10 = to_categorical(y_test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f18b6663",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMiwyO8XiWsK",
    "outputId": "b0ae23fe-808a-43c7-fd3b-6b6e4cbeedaf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 6s 3ms/step - loss: 0.9123 - accuracy: 0.5654\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.8565 - accuracy: 0.6011\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.8410 - accuracy: 0.6132\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.8318 - accuracy: 0.6196\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.8255 - accuracy: 0.6228\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.8208 - accuracy: 0.6253\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.8159 - accuracy: 0.6285\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.8125 - accuracy: 0.6298\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.8090 - accuracy: 0.6326\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.8078 - accuracy: 0.6340\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8053 - accuracy: 0.6352\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8034 - accuracy: 0.6359\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8020 - accuracy: 0.6360\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.8005 - accuracy: 0.6390\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7997 - accuracy: 0.6386\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.7980 - accuracy: 0.6381\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.7974 - accuracy: 0.6403: 0s - loss: 0.7967 - accu\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.7964 - accuracy: 0.6433\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.7955 - accuracy: 0.6411\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.7947 - accuracy: 0.6421\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8841 - accuracy: 0.5947\n",
      "Test loss: 0.8841245174407959\n",
      "Test accuracy: 0.5947499871253967\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100,  input_shape=(3000,)),\n",
    "    tf.keras.layers.Dense(10 ),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with cross-entropy loss and Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model with a batch size of 32 and for 10 epochs\n",
    "model.fit(X_train_10, y_train_10, batch_size=32, epochs=20)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_10, y_test_10)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae6182",
   "metadata": {},
   "source": [
    "### Q) What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section.\n",
    "\n",
    "### A) On comparing the performance of Simple Models to FNN models, we can observe that the on the entire dataset, Feedforward Neural Network outperforms the Perceptron Model and performs similar to SVM. With more tuning and epochs, the FNN might have an edge over both the Simple Models. FNN capture the complex relations better as they have non-linear decision  boundaries. As the features are well crafted, they perform almost similarly. \n",
    "\n",
    "### The accuracies are as follows:\n",
    "1. Perceptron - 0.5959180245254494\n",
    "2. SVM - 0.6500923903913993\n",
    "3. FNN(All the words) - 0.6479926109313965\n",
    "4. FNN(For 10 words) - 0.5947499871253967\n",
    "\n",
    "### We can also see that the FNN with average of vectors of all the words in the review performs better than FNN for concatenated 10 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e89e8d4",
   "metadata": {
    "id": "Ns7kbw99a0Tp"
   },
   "source": [
    "### 5. Recurrent Neural Networks (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788dbd75",
   "metadata": {
    "id": "Lz2gjRi8a5jo"
   },
   "source": [
    "### (a) (10 points)\n",
    "Train a simple RNN for sentiment analysis. You can consider an RNN cell\n",
    "with the hidden state size of 20. To feed your data into our RNN, limit\n",
    "the maximum review length to 20 by truncating longer reviews and padding\n",
    "shorter reviews with a null value (0). Report accuracy values on the testing\n",
    "split for your RNN model.\n",
    "What do you conclude by comparing accuracy values you obtain with\n",
    "those obtained with feedforward neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66ba3a0c",
   "metadata": {
    "id": "XFeLHOb4wp41"
   },
   "outputs": [],
   "source": [
    "def get_sentence_embeddings_RNN(df, model, num_words=10):\n",
    "    # Get the embeddings for each sentence in the dataframe\n",
    "    # df.dropna(subset=['review_body'], inplace=True)\n",
    "\n",
    "    embeddings = []\n",
    "    for sentence in df:\n",
    "        if type(sentence) != float:\n",
    "            words = sentence.split()[:num_words]  # Take the first num_words words in the sentence\n",
    "            sentence_embeddings = []\n",
    "            for word in words:\n",
    "                if word in model:\n",
    "                    sentence_embeddings.append(model[word])\n",
    "                else:\n",
    "                    sentence_embeddings.append(np.zeros(300))\n",
    "          # Pad the sentence_embeddings list with zero vectors if necessary\n",
    "            while len(sentence_embeddings) < num_words:\n",
    "                sentence_embeddings.append(np.zeros(300))\n",
    "            embeddings.append(sentence_embeddings)\n",
    "        else:\n",
    "            sentence_embeddings = []\n",
    "            while len(sentence_embeddings) < num_words:\n",
    "                sentence_embeddings.append(np.zeros(300))\n",
    "            embeddings.append(sentence_embeddings)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a659935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(complete_data['review_body'], complete_data['output_class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f914efc",
   "metadata": {
    "id": "fOrC9AdtjsaR"
   },
   "outputs": [],
   "source": [
    "X_train = get_sentence_embeddings_RNN(X_train, wv, 20)\n",
    "X_test = get_sentence_embeddings_RNN(X_test, wv, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6557f4c4",
   "metadata": {
    "id": "pPjMlNozyagJ"
   },
   "outputs": [],
   "source": [
    "y_train= to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "086cf6a5",
   "metadata": {
    "id": "yPft1Wm4yji2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 10s 5ms/step - loss: 0.9394 - accuracy: 0.5364\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.8652 - accuracy: 0.5840\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8497 - accuracy: 0.5919\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8380 - accuracy: 0.6030\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8260 - accuracy: 0.6164\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8196 - accuracy: 0.6207\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.8117 - accuracy: 0.6261\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.8062 - accuracy: 0.6301\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8013 - accuracy: 0.6345\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.7953 - accuracy: 0.6367\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.7916 - accuracy: 0.6410\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7859 - accuracy: 0.6415\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.7848 - accuracy: 0.6425\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.7811 - accuracy: 0.6468\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.7771 - accuracy: 0.6475\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7738 - accuracy: 0.6518\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.7725 - accuracy: 0.6507\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.7685 - accuracy: 0.6531\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.7693 - accuracy: 0.6539\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.7646 - accuracy: 0.6554\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8310 - accuracy: 0.6162\n",
      "Test loss: 0.8309546709060669\n",
      "Test accuracy: 0.6162499785423279\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(20, input_shape=(20, 300)),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25381999",
   "metadata": {},
   "source": [
    "### Q) What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models.\n",
    "\n",
    "### A) The test accuracies are as follows:\n",
    "1. RNN(For 20 words of the reviews) - 0.6162499785423279\n",
    "2. FNN(For entire corpus) - 0.6479926109313965\n",
    "3. FNN(For 10 words) - 0.5947499871253967\n",
    "\n",
    "### From these results we can see that the FNN trained on entire dataset outperforms the FNN(For 10 words) and RNN(For 20 words). As the RNN is not trained on the entire corpus, but still manages to get an accuracy close to FNN, with more data it might perform better. \n",
    "\n",
    "### FNN can handle large data better. Meanwhile, RNN can perform better on complex data as they have a memory component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480ec5e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "lOEM3QyzzWL6",
    "outputId": "fe3e6938-a070-4c56-e9ed-9f32064252f3"
   },
   "source": [
    "### (b) (10 points)\n",
    "Repeat part (a) by considering a gated recurrent unit cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4487532",
   "metadata": {
    "id": "XpidgA072UNa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 11s 6ms/step - loss: 0.9136 - accuracy: 0.5491\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7884 - accuracy: 0.6414\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7623 - accuracy: 0.6560\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7440 - accuracy: 0.6670: 0s - l\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7278 - accuracy: 0.6741\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.7159 - accuracy: 0.6816\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.7044 - accuracy: 0.6877\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.6956 - accuracy: 0.6920\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.6851 - accuracy: 0.6972\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.6762 - accuracy: 0.7043\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.6676 - accuracy: 0.7070\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.6599 - accuracy: 0.7118\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.6531 - accuracy: 0.7152\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.6459 - accuracy: 0.7191\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.6385 - accuracy: 0.7223\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.6320 - accuracy: 0.7262\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.6268 - accuracy: 0.7289\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.6200 - accuracy: 0.7333\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.6140 - accuracy: 0.7372\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.6076 - accuracy: 0.7410\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.7605 - accuracy: 0.6643\n",
      "Test loss: 0.7605224847793579\n",
      "Test accuracy: 0.6643333435058594\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(20, 300)),\n",
    "    tf.keras.layers.GRU(20),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce518e9",
   "metadata": {},
   "source": [
    "### (c) (10 points)\n",
    "Repeat part (a) by considering an LSTM unit cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7117719",
   "metadata": {
    "id": "62XHN-4vzWg_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 11s 6ms/step - loss: 0.9126 - accuracy: 0.5502\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.7988 - accuracy: 0.6355\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.7701 - accuracy: 0.6516\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.7529 - accuracy: 0.6617\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 0.7379 - accuracy: 0.6697\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 0.7251 - accuracy: 0.6758\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.7127 - accuracy: 0.6825\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 10s 6ms/step - loss: 0.7006 - accuracy: 0.6902\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.6884 - accuracy: 0.6961\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.6793 - accuracy: 0.7011\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.6685 - accuracy: 0.7063\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.6603 - accuracy: 0.7115\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.6514 - accuracy: 0.7175\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.6433 - accuracy: 0.7208\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 14s 9ms/step - loss: 0.6352 - accuracy: 0.7250\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 16s 11ms/step - loss: 0.6264 - accuracy: 0.7292\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.6183 - accuracy: 0.7340\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 17s 11ms/step - loss: 0.6125 - accuracy: 0.7360\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 13s 8ms/step - loss: 0.6052 - accuracy: 0.7394\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.5986 - accuracy: 0.7434\n",
      "375/375 [==============================] - 3s 4ms/step - loss: 0.7898 - accuracy: 0.6569\n",
      "Test loss: 0.7897895574569702\n",
      "Test accuracy: 0.6569166779518127\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(20, 300)),\n",
    "    tf.keras.layers.LSTM(20),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200c6fa",
   "metadata": {},
   "source": [
    "### Q) What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN.\n",
    "\n",
    "### A) On comparing the accuracies between GRU, LSTM and simple RNN, I observed that the LSTM and GRU perform better than the Simple RNN. The test accuracy between GRU and LSTM is almost the same. LSTM outperforms by 0.01. LSTM retain memory for longer.\n",
    "\n",
    "#### The accuracies are as follows:\n",
    "1. Simple RNN - 0.6162499785423279\n",
    "2. GRU - 0.6643333435058594\n",
    "3. LSTM - 0.6569166779518127"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249fd90",
   "metadata": {},
   "source": [
    "References: \n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    https://www.tensorflow.org/tutorials/quickstart/beginner\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
